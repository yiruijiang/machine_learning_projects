{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55adcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e5f41",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Gradients - Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba8acb",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d39b04",
   "metadata": {},
   "source": [
    "* Vanilla SGD\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
    "$$\n",
    "\n",
    "* SGD with Momentum\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{t+1} &= \\mu \\cdot v_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t) \\\\\\\\\n",
    "\\theta_{t+1} &= \\theta_t + v_{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Nesterov Accelerated Gradient (NAG)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{t+1} &= \\mu \\cdot v_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t + \\mu \\cdot v_t) \\\\\\\\\n",
    "\\theta_{t+1} &= \\theta_t + v_{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Weight Decay (L2 Regularization)\n",
    "$$\n",
    "\\nabla_\\theta \\mathcal{L}_{\\text{reg}} = \\nabla_\\theta \\mathcal{L} + \\lambda \\cdot \\theta\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "205e527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions after training:\n",
      " [[0.5707908  0.42920917]\n",
      " [0.6387993  0.36120063]\n",
      " [0.7016642  0.29833582]\n",
      " [0.7577372  0.2422628 ]]\n",
      "Final weights:\n",
      " [[0.34792542]\n",
      " [0.06284701]]\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0]).reshape(-1, 1)\n",
    "y = torch.tensor([0.0, 0.0, 1.0, 1.0]).reshape(-1, 1)\n",
    "cls_target = torch.cat([y, 1 - y], dim=1)  # One-hot target: [1,0] or [0,1]\n",
    "\n",
    "# Model: Linear -> Softmax\n",
    "layer = nn.Sequential(\n",
    "    nn.Linear(1, 2, bias=False),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-2\n",
    "v = torch.zeros_like(layer[0].weight)\n",
    "\n",
    "# Loss function: manual cross-entropy\n",
    "def loss_fn(pred, target):\n",
    "    return -(target * torch.log(pred)).sum(dim=1).mean()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(100):\n",
    "    # 1. Lookahead step\n",
    "    lookahead = layer[0].weight + momentum * v\n",
    "    with torch.no_grad():\n",
    "        layer[0].weight.copy_(lookahead)\n",
    "\n",
    "    # 2. Forward pass and loss\n",
    "    pred = layer(x)\n",
    "    loss = loss_fn(pred, cls_target)\n",
    "\n",
    "    # 3. Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Weight decay (L2 regularization)\n",
    "    grad = layer[0].weight.grad + weight_decay * layer[0].weight\n",
    "\n",
    "    # 5. Momentum update\n",
    "    v = momentum * v - lr * grad\n",
    "\n",
    "    # 6. Final weight update (Nesterov style)\n",
    "    with torch.no_grad():\n",
    "        layer[0].weight.copy_(lookahead - momentum * v + v)\n",
    "\n",
    "    # 7. Reset gradient\n",
    "    layer[0].weight.grad.zero_()\n",
    "\n",
    "# Output\n",
    "with torch.no_grad():\n",
    "    print(\"Predictions after training:\\n\", layer(x).squeeze().numpy())\n",
    "    print(\"Final weights:\\n\", layer[0].weight.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1621e",
   "metadata": {},
   "source": [
    "## Adam (Adaptive moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d13fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f08c00b1",
   "metadata": {},
   "source": [
    "## Weight initilization #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca12313",
   "metadata": {},
   "source": [
    "kaiming weight initialization makes sure vairance through layers stays stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7f82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891812de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
